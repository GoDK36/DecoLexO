{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitbaseconda7ccc1197ea424c04aa8e970980241a68",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 20)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m20\u001b[0m\n\u001b[1;33m    def merge(self):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def check_info(self, word):\n",
    "        sem_rgx = re.compile (r'[Q][A-Z]{3}')  # semantic tagset\n",
    "        syn_rgx = re.compile (r'[Y][A-Z]{3}')  # syntactic tagset\n",
    "        dom_rgx = re.compile (r'[X]{1}[ABCDEFGHIJKLMNOPQRSTUVWYZ]{3}')  # domain tagset\n",
    "        ent_rgx = re.compile (r'[X]{2}[A-Z]{2}')  # entity tagset\n",
    "        mor_rgx = re.compile (r'[A-Z]{3}')  # morph tagset\n",
    "\n",
    "        if sem_rgx.match (word):\n",
    "            return 'SenInfo'\n",
    "        elif syn_rgx.match (word):\n",
    "            return 'SynInfo'\n",
    "        elif dom_rgx.match (word):\n",
    "            return 'DomInfo'\n",
    "        elif ent_rgx.match (word):\n",
    "            return 'EntInfo'\n",
    "        elif mor_rgx.match (word):\n",
    "            return 'MorInfo'\n",
    "\n",
    "\n",
    "    def merge(self):\n",
    "\n",
    "        #들어오는 데이터를 병합할 변수\n",
    "        global merge_data\n",
    "        global cnt\n",
    "\n",
    "        #처음들어왔을 때는 병합할 필요가 없으므로 열린 파일을 merge_data에 저장\n",
    "        if (self.dataFrame_Tab.currentIndex()-1) == 0:\n",
    "            merge_data = handle_df_list[0]\n",
    "        \n",
    "        #두 번째부터는 들어오는 데이터를 data변수에 저장하고 \n",
    "        #이전에 저장해둔 merge_data와 data를 concat으로 병합한 뒤\n",
    "        #두 데이터를 sort_values로 정렬을 시켜준다.\n",
    "        else:\n",
    "            data = handle_df_list[self.dataFrame_Tab.currentIndex()-1]\n",
    "            merge_data = pd.concat([merge_data,data],ignore_index=True)\n",
    "            merge_data = merge_data.sort_values(by='Lemma')\n",
    "            merge_data = merge_data.reset_index()\n",
    "            cnt += 1\n",
    "\n",
    "        #사용자가 입력할 col_name.\n",
    "        #gui상에서 .text()로 입력받는다.\n",
    "        col_data = 'MorInfo1'\n",
    "\n",
    "        #우선시 되는 데이터 : my_data, 지워져도 되는 데이터 del_data \n",
    "        #gui상에서 .text()로 입력받는다.\n",
    "        my_data = 'ZND'\n",
    "        del_data = 'ZNZ'\n",
    "\n",
    "        \n",
    "        #파일들을 1개 이상 입력 받으면 실행된\n",
    "        if cnt != 0:\n",
    "            #중복 단어들을 저장하는 리스트\n",
    "            reduplication = []\n",
    "            #i는 처음부터 끝에서 두번째에 있는 단어들까지 돌아가고\n",
    "            #j는 i번째에 있는 단어 바로 다음 단어랑 비교를 한다.\n",
    "            #왜냐하면 정렬이 되어있는 상태기 때문에 바로 전 후의 단어들을 비교해서\n",
    "            #앞글자가 다르면 패스를 해서 시간을 단축시키기 위함이다.\n",
    "            for i in range(0,len(merge_data)-1):\n",
    "                for j in range(i+1, i+cnt+1):\n",
    "                    #first에는 i번째 단어를 음절별로 나누어서 저장하고\n",
    "                    #second에는 j번째(i다음 단어)를 음절로 나누어서 저장한다.\n",
    "                    first = Divide(merge_data.loc[i,'Lemma'])\n",
    "                    second = Divide(merge_data.loc[j,'Lemma'])\n",
    "\n",
    "                    #저장한 단어들의 앞글자가 다르면 i를 1증가시켜 다음 단어로 넘어간다.\n",
    "                    if first[0] != second[0]:\n",
    "                        break\n",
    "                    \n",
    "                    #만약 단어의 앞글자가 같다면 아래 코드를 실행한다.\n",
    "                    elif first[0] == second[0]:\n",
    "                        #앞글자는 같지만 단어가 다르면 for문을 종료한다.\n",
    "                        if first != second:\n",
    "                            break\n",
    "                        \n",
    "                        #앞글자가 같고 단어까지 같으면 입력 받은 mydata가 i번째에 있는지 아님 j(i+1)번째 있는지 확인한다.\n",
    "                        elif first == second:\n",
    "                            if my_data in merge_data.loc[i,col_data] and my_data in merge_data.loc[j,col_data]:\n",
    "                                pass\n",
    "                            elif my_data in merge_data.loc[i,col_data] and del_data in merge_data.loc[j,col_data]:\n",
    "                                #reduplication.append(merge_data.iloc[i].values.tolist())\n",
    "                                #reduplication.append(merge_data.iloc[j].values.tolist())\n",
    "\n",
    "                                # stem_data에는 i번째 단어 정보를 리스트형태로 저장하고\n",
    "                                # follow_data에는 j번째 (i+1)번째 단어 정보를 리스트 형태로 저장한다.\n",
    "                                # 변수 x는 follow_data를 돌면서 follow_data요소가 stem_data에 정보가 없으면 \n",
    "                                # 그 정보들 check_info()함수에 넘겨서 정보를 col에 저장해준다(ex. MorInfo)\n",
    "                                # y는 merge_data의 colum 정보들을 돌면서\n",
    "                                # check_info로 입력받은 정보가 들어있는 column이 처음으로 빈칸이 나오는 장소에\n",
    "                                # stem_data에 들어있지 않은 정보(follow_data)를 merge_data에 넣어준다.\n",
    "                                stem_data1 = merge_data.iloc[i].values.tolist()\n",
    "                                follow_data1 = merge_data.iloc[j].values.tolist()\n",
    "                                for x in range(4, len(follow_data1)-1):\n",
    "                                    if follow_data1[x] not in stem_data1:\n",
    "                                        col = self.check_info(follow_data1[x])\n",
    "                                        col_nme = merge_data.columns\n",
    "                                        for y in range(4,len(col_nme)):\n",
    "                                           #print()\n",
    "                                            if col in col_nme[y] and merge_data.loc[j, col_nme[y]] == '':\n",
    "                                                merge_data.loc[j, col_nme[y]] = follow_data1[x]\n",
    "                                                break\n",
    "                                \n",
    "                            elif del_data in merge_data.loc[i,col_data] and my_data in merge_data.loc[j,col_data]:\n",
    "                                #reduplication.append(merge_data.iloc[i].values.tolist())\n",
    "                                #reduplication.append(merge_data.iloc[j].values.tolist())\n",
    "                                stem_data2 = merge_data.iloc[j].values.tolist()\n",
    "                                follow_data2 = merge_data.iloc[i].values.tolist()\n",
    "                                for x in range(4, len(follow_data2)-1):\n",
    "                                    if follow_data2[x] not in stem_data2:\n",
    "                                        col = self.check_info(follow_data2[x])\n",
    "                                        col_nme = merge_data.columns\n",
    "                                        for y in range(4,len(col_nme)):\n",
    "                                           #print()\n",
    "                                            if col in col_nme[y] and merge_data.loc[j, col_nme[y]] == '':\n",
    "                                                merge_data.loc[j, col_nme[y]] = follow_data2[x]\n",
    "                                                break\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def check_info(word):\n",
    "    sem_rgx = re.compile (r'[Q][A-Z]{3}')  # semantic tagset\n",
    "    syn_rgx = re.compile (r'[Y][A-Z]{3}')  # syntactic tagset\n",
    "    dom_rgx = re.compile (r'[X]{1}[ABCDEFGHIJKLMNOPQRSTUVWYZ]{3}')  # domain tagset\n",
    "    ent_rgx = re.compile (r'[X]{2}[A-Z]{2}')  # entity tagset\n",
    "    mor_rgx = re.compile (r'[A-Z]{3}')  # morph tagset\n",
    "\n",
    "    if sem_rgx.match (word):\n",
    "        return 'SenInfo'\n",
    "    elif syn_rgx.match (word):\n",
    "        return 'SynInfo'\n",
    "    elif dom_rgx.match (word):\n",
    "        return 'DomInfo'\n",
    "    elif ent_rgx.match (word):\n",
    "        return 'EntInfo'\n",
    "    elif mor_rgx.match (word):\n",
    "        return 'MorInfo'\n",
    "\n",
    "def column_name(df):\n",
    "    # 첫 행 살리기\n",
    "    first = list (df.columns)\n",
    "    if first[0] == 0:\n",
    "        pass\n",
    "    else:\n",
    "        df.loc[0] = first\n",
    "        for val in first:\n",
    "            if 'Unnamed' in val:\n",
    "                x = first.index (val)\n",
    "                first[x] = np.nan\n",
    "        df.loc[0] = first\n",
    "    df = df.fillna ('')\n",
    "    sem_rgx = re.compile (r'[Q][A-Z]{3}')  # semantic tagset\n",
    "    syn_rgx = re.compile (r'[Y][A-Z]{3}')  # syntactic tagset\n",
    "    dom_rgx = re.compile (r'[X]{1}[ABCDEFGHIJKLMNOPQRSTUVWYZ]{3}')  # domain tagset\n",
    "    ent_rgx = re.compile (r'[X]{2}[A-Z]{2}')  # entity tagset\n",
    "    mor_rgx = re.compile (r'[A-Z]{3}')  # morph tagset\n",
    "\n",
    "    # 컬럼의 총 개수를 l에 저장한다.\n",
    "    # 컬럼의 개수 만큼 lemma와 category뒤에 lemma와 category개수인 2를 뺀만큼\n",
    "    # ''를 추가해 주어 해당 컬럼 개수 만큼의 리스트 col_nme을 만들어 준다.\n",
    "    l = len (df.columns)\n",
    "    col_nme = ['Lemma', 'Category']\n",
    "    for i in range (l - 2):\n",
    "        col_nme.append ('')\n",
    "    # sem =>SemInfo 뒤에 붙을 숫자\n",
    "    # syn =>SynInfo 뒤에 붙을 숫자\n",
    "    # dom =>DomInfo 뒤에 붙을 숫자\n",
    "    # ent =>EntInfo 뒤에 붙을 숫자\n",
    "    # mor =>MorInfo 뒤에 붙을 숫자\n",
    "    sem = 1\n",
    "    syn = 1\n",
    "    dom = 1\n",
    "    ent = 1\n",
    "    mor = 1\n",
    "\n",
    "    # x를 컬럼의 개수 만큼의 숫자로 지정해 준다.\n",
    "    # col_val은 해당 df의 열을 리스트화 시켜준 것이다.\n",
    "    for x in range (0, l):\n",
    "        col_val = df.iloc[:, x].tolist ()\n",
    "        # cnt가 0이면 일치하는 값을 못 찾았다는 의미로 해석(ex 모두 빈칸인 열을 만났을 때)\n",
    "        # 밑에서 cnt == 0 일때 앞에 정보를 보고 빈칸의 정보를 수정할 때 사용한다.\n",
    "        cnt = 0\n",
    "        # k로 col_val의 리스트 요소들을 하나씩 지정해주면서\n",
    "        # k가 sem_rgx, syn_rgx, dom_rgx, ent_rgx, mor_rgx에 해당되면\n",
    "        # 컬럼에 일치하는 값이 있었다는 의미로 cnt를 1 증가시켜 주고\n",
    "        # Info뒤에 붙을 숫자를 1씩 증가시켜 주고\n",
    "        # 비효율적인 탐색을 막기 위해 바로 break시켜준다.\n",
    "        for k in col_val:\n",
    "            if sem_rgx.match (k):\n",
    "                col_nme[x] = 'SemInfo' + str (sem)\n",
    "                sem += 1\n",
    "                cnt += 1\n",
    "                break\n",
    "\n",
    "            elif syn_rgx.match (k):\n",
    "                col_nme[x] = 'SynInfo' + str (syn)\n",
    "                syn += 1\n",
    "                cnt += 1\n",
    "                break\n",
    "\n",
    "            elif dom_rgx.match (k):\n",
    "                col_nme[x] = 'DomInfo' + str (dom)\n",
    "                dom += 1\n",
    "                cnt += 1\n",
    "                break\n",
    "\n",
    "            elif ent_rgx.match (k):\n",
    "                col_nme[x] = 'EntInfo' + str (ent)\n",
    "                ent += 1\n",
    "                cnt += 1\n",
    "                break\n",
    "\n",
    "            elif mor_rgx.match (k):\n",
    "                col_nme[x] = 'MorInfo' + str (mor)\n",
    "                mor += 1\n",
    "                cnt += 1\n",
    "                break\n",
    "\n",
    "        # 만약 위에서 일치하는 값을 못찾았을 때(ex 모두 빈칸인 열이었을 때)\n",
    "        # cnt는 0이므로 앞에 col_nme의 정보를 보고\n",
    "        # 해당 정보와 일치하는 정보의 Info숫자를 증가시켜준 값을 해당 리스트 위치에 저장해준다.\n",
    "        if cnt == 0:\n",
    "            if 'Sem' in col_nme[x - 1]:\n",
    "                col_nme[x] = 'SemInfo' + str (sem)\n",
    "                sem += 1\n",
    "\n",
    "            elif 'Syn' in col_nme[x - 1]:\n",
    "                col_nme[x] = 'SynInfo' + str (syn)\n",
    "                syn += 1\n",
    "\n",
    "            elif 'Dom' in col_nme[x - 1]:\n",
    "                col_nme[x] = 'DomInfo' + str (dom)\n",
    "                dom += 1\n",
    "\n",
    "            elif 'Ent' in col_nme[x - 1]:\n",
    "                col_nme[x] = 'EntInfo' + str (ent)\n",
    "                ent += 1\n",
    "\n",
    "            elif 'Mor' in col_nme[x - 1]:\n",
    "                col_nme[x] = 'MorInfo' + str (mor)\n",
    "                mor += 1\n",
    "\n",
    "    df.columns = col_nme\n",
    "\n",
    "    return df\n",
    "\n",
    "def Divide(korean_word):\n",
    "    CHOSUNG_LIST = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    # 중성 리스트. 00 ~ 20\n",
    "    JUNGSUNG_LIST = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ',\n",
    "                     'ㅣ']\n",
    "    # 종성 리스트. 00 ~ 27 + 1(1개 없음)\n",
    "    JONGSUNG_LIST = [' ', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ',\n",
    "                     'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    r_lst = []\n",
    "    for w in list(korean_word.strip()):\n",
    "        ## 영어인 경우 구분해서 작성함.\n",
    "        if '가'<=w<='힣':\n",
    "            ## 588개 마다 초성이 바뀜.\n",
    "            ch1 = (ord(w) - ord('가'))//588\n",
    "            ## 중성은 총 28가지 종류\n",
    "            ch2 = ((ord(w) - ord('가')) - (588*ch1)) // 28\n",
    "            ch3 = (ord(w) - ord('가')) - (588*ch1) - 28*ch2\n",
    "            r_lst.append([CHOSUNG_LIST[ch1], JUNGSUNG_LIST[ch2], JONGSUNG_LIST[ch3]])\n",
    "        else:\n",
    "            r_lst.append([w])\n",
    "    return r_lst\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "가\nnan\n"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-92bd792b8000>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerge_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m \u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-92bd792b8000>\u001b[0m in \u001b[0;36mmerge\u001b[1;34m()\u001b[0m\n\u001b[0;32m     76\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mfollow_data1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstem_data1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfollow_data1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                                 \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_info\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfollow_data1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m                                 \u001b[0mcol_nme\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                                 \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcol_nme\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-100abe2ff07d>\u001b[0m in \u001b[0;36mcheck_info\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mmor_rgx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34mr'[A-Z]{3}'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# morph tagset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0msem_rgx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m'SenInfo'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0msyn_rgx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "def merge():\n",
    "    # 들어오는 데이터를 병합할 변수\n",
    "\n",
    "    merge_data = pd.read_csv(r\"E:\\Programming\\python\\NLP\\DecoLexO\\DecoLexO\\example\\Merge1.csv\", header=None, encoding='utf-8-sig')\n",
    "    merge_data = column_name(merge_data)\n",
    "    data_files = [r'E:\\Programming\\python\\NLP\\DecoLexO\\DecoLexO\\example\\Merge2.csv', r\"E:\\Programming\\python\\NLP\\DecoLexO\\DecoLexO\\example\\Merge3.csv\"]\n",
    "    \n",
    "    data_df_list = []\n",
    "    for i in data_files:\n",
    "        temp_data = pd.read_csv(i, header=None, encoding='utf-8-sig')\n",
    "        temp_data = column_name(temp_data)\n",
    "        data_df_list.append(temp_data)\n",
    "    # 두 번째부터는 들어오는 데이터를 data변수에 저장하고\n",
    "    # 이전에 저장해둔 merge_data와 data를 concat으로 병합한 뒤\n",
    "    # 두 데이터를 sort_values로 정렬을 시켜준다.\n",
    "    for data in data_df_list:\n",
    "        merge_data = pd.concat ([merge_data, data], ignore_index=True)\n",
    "    merge_data = merge_data.sort_values (by='Lemma')\n",
    "    merge_data = merge_data.reset_index ()\n",
    "    # merge_data = merge_data.dropna(axis=0)\n",
    "\n",
    "    # 사용자가 입력할 col_name.\n",
    "    # gui상에서 .text()로 입력받는다.\n",
    "    col_data = 'MorInfo1'\n",
    "\n",
    "    # 우선시 되는 데이터 : my_data, 지워져도 되는 데이터 del_data\n",
    "    # gui상에서 .text()로 입력받는다.\n",
    "    rules = [['ZNZ','ZNW'],['ZNE','ZNW']]\n",
    "    for rule in rules:\n",
    "        my_data = rule[0]\n",
    "        del_data = rule[1]\n",
    "\n",
    "        # 중복 단어들을 저장하는 리스트\n",
    "        reduplication = []\n",
    "        # i는 처음부터 끝에서 두번째에 있는 단어들까지 돌아가고\n",
    "        # j는 i번째에 있는 단어 바로 다음 단어랑 비교를 한다.\n",
    "        # 왜냐하면 정렬이 되어있는 상태기 때문에 바로 전 후의 단어들을 비교해서\n",
    "        # 앞글자가 다르면 패스를 해서 시간을 단축시키기 위함이다.\n",
    "\n",
    "        for i in range (0, len(merge_data) - 1):\n",
    "            for j in range (i + 1, i + 2):\n",
    "                # first에는 i번째 단어를 음절별로 나누어서 저장하고\n",
    "                # second에는 j번째(i다음 단어)를 음절로 나누어서 저장한다.\n",
    "                first = merge_data.loc[i, 'Lemma']\n",
    "                second = merge_data.loc[j, 'Lemma']\n",
    "                print(first[0])\n",
    "\n",
    "                # # 저장한 단어들의 앞글자가 다르면 i를 1증가시켜 다음 단어로 넘어간다.\n",
    "                # if first[0] != second[0]:\n",
    "                #     break\n",
    "\n",
    "                # # 만약 단어의 앞글자가 같다면 아래 코드를 실행한다.\n",
    "                # elif first[0] == second[0]:\n",
    "                #     # 앞글자는 같지만 단어가 다르면 for문을 종료한다.\n",
    "                #     if first != second:\n",
    "                #         break\n",
    "\n",
    "                # 앞글자가 같고 단어까지 같으면 입력 받은 mydata가 i번째에 있는지 아님 j(i+1)번째 있는지 확인한다.\n",
    "                if first == second:\n",
    "                    if my_data in merge_data.loc[i, col_data] and my_data in merge_data.loc[j, col_data]:\n",
    "                        pass\n",
    "                    elif my_data in merge_data.loc[i, col_data] and del_data in merge_data.loc[j, col_data]:\n",
    "                        # reduplication.append(merge_data.iloc[i].values.tolist())\n",
    "                        # reduplication.append(merge_data.iloc[j].values.tolist())\n",
    "\n",
    "                        # stem_data에는 i번째 단어 정보를 리스트형태로 저장하고\n",
    "                        # follow_data에는 j번째 (i+1)번째 단어 정보를 리스트 형태로 저장한다.\n",
    "                        # 변수 x는 follow_data를 돌면서 follow_data요소가 stem_data에 정보가 없으면\n",
    "                        # 그 정보들 check_info()함수에 넘겨서 정보를 col에 저장해준다(ex. MorInfo)\n",
    "                        # y는 merge_data의 colum 정보들을 돌면서\n",
    "                        # check_info로 입력받은 정보가 들어있는 column이 처음으로 빈칸이 나오는 장소에\n",
    "                        # stem_data에 들어있지 않은 정보(follow_data)를 merge_data에 넣어준다.\n",
    "                        stem_data1 = merge_data.iloc[i].values.tolist ()\n",
    "                        follow_data1 = merge_data.iloc[j].values.tolist ()\n",
    "                        for x in range (4, len (follow_data1) - 1):\n",
    "                            if follow_data1[x] not in stem_data1:\n",
    "                                print(follow_data1[x])\n",
    "                                col = check_info (follow_data1[x])\n",
    "                                col_nme = merge_data.columns\n",
    "                                for y in range (4, len (col_nme)):\n",
    "                                    if col in col_nme[y] and merge_data.loc[j, col_nme[y]] == '':\n",
    "                                        merge_data.loc[j, col_nme[y]] = follow_data1[x]\n",
    "                                        break\n",
    "\n",
    "                    elif del_data in merge_data.loc[i, col_data] and my_data in merge_data.loc[j, col_data]:\n",
    "                        # reduplication.append(merge_data.iloc[i].values.tolist())\n",
    "                        # reduplication.append(merge_data.iloc[j].values.tolist())\n",
    "                        stem_data2 = merge_data.iloc[j].values.tolist ()\n",
    "                        follow_data2 = merge_data.iloc[i].values.tolist ()\n",
    "                        for x in range (4, len (follow_data2) - 1):\n",
    "                            if follow_data2[x] not in stem_data2:\n",
    "                                col = check_info (follow_data2[x])\n",
    "                                col_nme = merge_data.columns\n",
    "                                for y in range (4, len (col_nme)):\n",
    "                                    if col in col_nme[y] and merge_data.loc[j, col_nme[y]] == '':\n",
    "                                        merge_data.loc[j, col_nme[y]] = follow_data2[x]\n",
    "                                        break\n",
    "                else:\n",
    "                    break\n",
    "    print(merge_data)\n",
    "\n",
    "merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "nan\n"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-e7a0abba78bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mfollow_data1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstem_data1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfollow_data1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m                                 \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_info\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfollow_data1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m                                 \u001b[0mcol_nme\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                                 \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcol_nme\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-100abe2ff07d>\u001b[0m in \u001b[0;36mcheck_info\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mmor_rgx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34mr'[A-Z]{3}'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# morph tagset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0msem_rgx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m'SenInfo'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0msyn_rgx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "\n",
    "# 들어오는 데이터를 병합할 변수\n",
    "\n",
    "merge_data = pd.read_csv(r\"E:\\Programming\\python\\NLP\\DecoLexO\\DecoLexO\\example\\Merge1.csv\", header=None, encoding='utf-8-sig')\n",
    "merge_data = column_name(merge_data)\n",
    "data_files = [r'E:\\Programming\\python\\NLP\\DecoLexO\\DecoLexO\\example\\Merge2.csv', r\"E:\\Programming\\python\\NLP\\DecoLexO\\DecoLexO\\example\\Merge3.csv\"]\n",
    "\n",
    "data_df_list = []\n",
    "for i in data_files:\n",
    "    temp_data = pd.read_csv(i, header=None, encoding='utf-8-sig')\n",
    "    temp_data = column_name(temp_data)\n",
    "    data_df_list.append(temp_data)\n",
    "# 두 번째부터는 들어오는 데이터를 data변수에 저장하고\n",
    "# 이전에 저장해둔 merge_data와 data를 concat으로 병합한 뒤\n",
    "# 두 데이터를 sort_values로 정렬을 시켜준다.\n",
    "for data in data_df_list:\n",
    "    merge_data = pd.concat ([merge_data, data], ignore_index=True)\n",
    "merge_data = merge_data.sort_values (by='Lemma')\n",
    "merge_data = merge_data.reset_index ()\n",
    "merge_data = merge_data.fillna('')\n",
    "del merge_data['index']\n",
    "\n",
    "# 우선시 되는 데이터 : my_data, 지워져도 되는 데이터 del_data\n",
    "# gui상에서 .text()로 입력받는다.\n",
    "rules = [['ZNZ','ZNW'],['ZNE','ZNW']]\n",
    "for rule in rules:\n",
    "    my_data = rule[0]\n",
    "    del_data = rule[1]\n",
    "\n",
    "    # 중복 단어들을 저장하는 리스트\n",
    "    reduplication = []\n",
    "    # i는 처음부터 끝에서 두번째에 있는 단어들까지 돌아가고\n",
    "    # j는 i번째에 있는 단어 바로 다음 단어랑 비교를 한다.\n",
    "    # 왜냐하면 정렬이 되어있는 상태기 때문에 바로 전 후의 단어들을 비교해서\n",
    "    # 앞글자가 다르면 패스를 해서 시간을 단축시키기 위함이다.\n",
    "\n",
    "    for i in range (0, len(merge_data) - 1):\n",
    "        for j in range (i + 1, i + 2):\n",
    "            # first에는 i번째 단어를 음절별로 나누어서 저장하고\n",
    "            # second에는 j번째(i다음 단어)를 음절로 나누어서 저장한다.\n",
    "            first = merge_data.loc[i, 'Lemma']\n",
    "            second = merge_data.loc[j, 'Lemma']\n",
    "\n",
    "            if first == second:\n",
    "                if my_data in merge_data.loc[i, 'MorInfo1'] and my_data in merge_data.loc[j, 'MorInfo1']:\n",
    "                        pass\n",
    "                if my_data in merge_data.loc[i, 'MorInfo1'] and del_data in merge_data.loc[j, 'MorInfo1']:\n",
    "                        # reduplication.append(merge_data.iloc[i].values.tolist())\n",
    "                        # reduplication.append(merge_data.iloc[j].values.tolist())\n",
    "\n",
    "                        # stem_data에는 i번째 단어 정보를 리스트형태로 저장하고\n",
    "                        # follow_data에는 j번째 (i+1)번째 단어 정보를 리스트 형태로 저장한다.\n",
    "                        # 변수 x는 follow_data를 돌면서 follow_data요소가 stem_data에 정보가 없으면\n",
    "                        # 그 정보들 check_info()함수에 넘겨서 정보를 col에 저장해준다(ex. MorInfo)\n",
    "                        # y는 merge_data의 colum 정보들을 돌면서\n",
    "                        # check_info로 입력받은 정보가 들어있는 column이 처음으로 빈칸이 나오는 장소에\n",
    "                        # stem_data에 들어있지 않은 정보(follow_data)를 merge_data에 넣어준다.\n",
    "                        stem_data1 = merge_data.iloc[i].values.tolist ()\n",
    "                        follow_data1 = merge_data.iloc[j].values.tolist ()\n",
    "                        for x in range (4, len (follow_data1) - 1):\n",
    "                            if follow_data1[x] not in stem_data1:\n",
    "                                print(follow_data1[x])\n",
    "                                col = check_info (follow_data1[x])\n",
    "                                col_nme = merge_data.columns\n",
    "                                for y in range (4, len (col_nme)):\n",
    "                                    if col in col_nme[y] and merge_data.loc[j, col_nme[y]] == '':\n",
    "                                        merge_data.loc[j, col_nme[y]] = follow_data1[x]\n",
    "                                        break\n",
    "\n",
    "                elif del_data in merge_data.loc[i, 'MorInfo1'] and my_data in merge_data.loc[j, 'MorInfo1']:\n",
    "                        # reduplication.append(merge_data.iloc[i].values.tolist())\n",
    "                        # reduplication.append(merge_data.iloc[j].values.tolist())\n",
    "                        stem_data2 = merge_data.iloc[j].values.tolist ()\n",
    "                        follow_data2 = merge_data.iloc[i].values.tolist ()\n",
    "                        for x in range (4, len (follow_data2) - 1):\n",
    "                            if follow_data2[x] not in stem_data2:\n",
    "                                col = check_info (follow_data2[x])\n",
    "                                col_nme = merge_data.columns\n",
    "                                for y in range (4, len (col_nme)):\n",
    "                                    if col in col_nme[y] and merge_data.loc[j, col_nme[y]] == '':\n",
    "                                        merge_data.loc[j, col_nme[y]] = follow_data2[x]\n",
    "                                        break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "print(merge_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Lemma Category MorInfo1 MorInfo2 MorInfo3 MorInfo4 MorInfo5 MorInfo6  \\\n0       가게     NS01      ZNZ      LEO      SLB      HAL      MCO            \n1       가게     NS01      ZNW                                                \n2       가게     NS01      ZNW                                                \n3       가게     NS01      ZNW                                                \n4       가게     NS01      ZNW                                                \n...    ...      ...      ...      ...      ...      ...      ...      ...   \n4731   지은이     NS01      ZNZ                                                \n4732   지은이     NS01      ZNZ                                                \n4733   지은이     NS01      ZNZ                                                \n4734   지은이     NS01      ZNZ                                                \n4735   지은이     NS01      ZNZ                                                \n\n     MorInfo7 MorInfo8  ... DomInfo4 DomInfo5 DomInfo6 DomInfo7 DomInfo8  \\\n0                       ...                                                \n1                       ...                                                \n2                       ...                                                \n3                       ...                                                \n4                       ...                                                \n...       ...      ...  ...      ...      ...      ...      ...      ...   \n4731                    ...                                                \n4732                    ...                                                \n4733                    ...                                                \n4734                    ...                                                \n4735                    ...                                                \n\n     DomInfo9 DomInfo10 DomInfo11 DomInfo12 DomInfo13  \n0                                                      \n1                                                      \n2                                                      \n3                                                      \n4                                                      \n...       ...       ...       ...       ...       ...  \n4731                                                   \n4732                                                   \n4733                                                   \n4734                                                   \n4735                                                   \n\n[4736 rows x 65 columns]\n     Lemma Category MorInfo1 MorInfo2 MorInfo3 MorInfo4 MorInfo5 MorInfo6  \\\n0       가게     NS01      ZNZ      LEO      SLB      HAL      MCO            \n1       가게     NS01      ZNW                                                \n2       가게     NS01      ZNW                                                \n3       가게     NS01      ZNW                                                \n4       가게     NS01      ZNW                                                \n...    ...      ...      ...      ...      ...      ...      ...      ...   \n4731   지은이     NS01      ZNZ                                                \n4732   지은이     NS01      ZNZ                                                \n4733   지은이     NS01      ZNZ                                                \n4734   지은이     NS01      ZNZ                                                \n4735   지은이     NS01      ZNZ                                                \n\n     MorInfo7 MorInfo8  ... DomInfo4 DomInfo5 DomInfo6 DomInfo7 DomInfo8  \\\n0                       ...                                                \n1                       ...                                                \n2                       ...                                                \n3                       ...                                                \n4                       ...                                                \n...       ...      ...  ...      ...      ...      ...      ...      ...   \n4731                    ...                                                \n4732                    ...                                                \n4733                    ...                                                \n4734                    ...                                                \n4735                    ...                                                \n\n     DomInfo9 DomInfo10 DomInfo11 DomInfo12 DomInfo13  \n0                                                      \n1                                                      \n2                                                      \n3                                                      \n4                                                      \n...       ...       ...       ...       ...       ...  \n4731                                                   \n4732                                                   \n4733                                                   \n4734                                                   \n4735                                                   \n\n[4736 rows x 65 columns]\n"
    }
   ],
   "source": [
    "def merge():\n",
    "    # 들어오는 데이터를 병합할 변수\n",
    "\n",
    "   # 들어오는 데이터를 병합할 변수\n",
    "\n",
    "    merge_data = pd.read_csv(r\"E:\\Programming\\python\\NLP\\DecoLexO\\DecoLexO\\example\\Merge1.csv\", header=None, encoding='utf-8-sig')\n",
    "    merge_data = column_name(merge_data)\n",
    "    data_files = [r'E:\\Programming\\python\\NLP\\DecoLexO\\DecoLexO\\example\\Merge2.csv', r\"E:\\Programming\\python\\NLP\\DecoLexO\\DecoLexO\\example\\Merge3.csv\"]\n",
    "\n",
    "    for i in data_files:\n",
    "        temp_data = pd.read_csv(i, header=None, encoding='utf-8-sig')\n",
    "        temp_data = column_name(temp_data)\n",
    "        data_df_list.append(temp_data)\n",
    "    # 두 번째부터는 들어오는 데이터를 data변수에 저장하고\n",
    "    # 이전에 저장해둔 merge_data와 data를 concat으로 병합한 뒤\n",
    "    # 두 데이터를 sort_values로 정렬을 시켜준다.\n",
    "    for data in data_df_list:\n",
    "        merge_data = pd.concat ([merge_data, data], ignore_index=True)\n",
    "    merge_data = merge_data.sort_values (by='Lemma')\n",
    "    merge_data = merge_data.reset_index ()\n",
    "    merge_data = merge_data.fillna('')\n",
    "    del merge_data['index']\n",
    "    cnt = 1\n",
    "    # 사용자가 입력할 col_name.\n",
    "    # gui상에서 .text()로 입력받는다.\n",
    "    col_data = 'MorInfo1'\n",
    "\n",
    "    # 우선시 되는 데이터 : my_data, 지워져도 되는 데이터 del_data\n",
    "    # gui상에서 .text()로 입력받는다.\n",
    "    rules = [['ZNZ','ZNW'],['ZNE','ZNW']]\n",
    "    for rule in rules:\n",
    "        my_data = rule[0]\n",
    "        del_data = rule[1]\n",
    "        for i in range (0, len(merge_data) - 1):\n",
    "            for j in range (i + 1, i + cnt + 1):\n",
    "                # first에는 i번째 단어를 음절별로 나누어서 저장하고\n",
    "                # second에는 j번째(i다음 단어)를 음절로 나누어서 저장한다.\n",
    "                first = Divide (merge_data.loc[i, 'Lemma'])\n",
    "                second = Divide (merge_data.loc[j, 'Lemma'])\n",
    "                if first[0] != second[0]:\n",
    "                    break\n",
    "                    # 만약 단어의 앞글자가 같다면 아래 코드를 실행한다.\n",
    "                else:\n",
    "                    # 앞글자는 같지만 단어가 다르면 for문을 종료한다.\n",
    "                    if first != second:\n",
    "                        break\n",
    "\n",
    "                    # 앞글자가 같고 단어까지 같으면 입력 받은 mydata가 i번째에 있는지 아님 j(i+1)번째 있는지 확인한다.\n",
    "                    else:\n",
    "                        if my_data == merge_data.loc[i, col_data] and del_data == merge_data.loc[j, col_data]:\n",
    "                            # reduplication.append(merge_data.iloc[i].values.tolist())\n",
    "                            # reduplication.append(merge_data.iloc[j].values.tolist())\n",
    "\n",
    "                            # stem_data에는 i번째 단어 정보를 리스트형태로 저장하고\n",
    "                            # follow_data에는 j번째 (i+1)번째 단어 정보를 리스트 형태로 저장한다.\n",
    "                            # 변수 x는 follow_data를 돌면서 follow_data요소가 stem_data에 정보가 없으면\n",
    "                            # 그 정보들 check_info()함수에 넘겨서 정보를 col에 저장해준다(ex. MorInfo)\n",
    "                            # y는 merge_data의 colum 정보들을 돌면서\n",
    "                            # check_info로 입력받은 정보가 들어있는 column이 처음으로 빈칸이 나오는 장소에\n",
    "                            # stem_data에 들어있지 않은 정보(follow_data)를 merge_data에 넣어준다.\n",
    "                            stem_data1 = merge_data.iloc[i].values.tolist ()\n",
    "                            follow_data1 = merge_data.iloc[j].values.tolist ()\n",
    "                            for x in range (4, len (follow_data1) - 1):\n",
    "                                if follow_data1[x] not in stem_data1:\n",
    "                                    col = check_info (follow_data1[x])\n",
    "                                    col_nme = merge_data.columns\n",
    "                                    for y in range (4, len (col_nme)):\n",
    "                                        if col in col_nme[y] and merge_data.loc[j, col_nme[y]] == '':\n",
    "                                            merge_data.loc[j, col_nme[y]] = follow_data1[x]\n",
    "                                            break\n",
    "\n",
    "                        elif my_data == merge_data.loc[i, col_data] and del_data == merge_data.loc[j, col_data]:\n",
    "                            # reduplication.append(merge_data.iloc[i].values.tolist())\n",
    "                            # reduplication.append(merge_data.iloc[j].values.tolist())\n",
    "                            stem_data2 = merge_data.iloc[j].values.tolist ()\n",
    "                            follow_data2 = merge_data.iloc[i].values.tolist ()\n",
    "                            for x in range (4, len (follow_data2) - 1):\n",
    "                                if follow_data2[x] not in stem_data2:\n",
    "                                    col = check_info (follow_data2[x])\n",
    "                                    col_nme = merge_data.columns\n",
    "                                    for y in range (4, len (col_nme)):\n",
    "                                        if col in col_nme[y] and merge_data.loc[j, col_nme[y]] == '':\n",
    "                                            merge_data.loc[j, col_nme[y]] = follow_data2[x]\n",
    "                                            break\n",
    "\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "                    \n",
    "        print(merge_data)\n",
    "        \n",
    "merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2389\n"
    },
    {
     "output_type": "error",
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'res_merge.csv'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-06714dd8c0bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmerge_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"res_merge.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_rep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8-sig'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m \u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RM'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-56-06714dd8c0bb>\u001b[0m in \u001b[0;36mmerge\u001b[1;34m()\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mmerge_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"index\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmerge_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"res_merge.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_rep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8-sig'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RM'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3201\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3202\u001b[0m         )\n\u001b[1;32m-> 3203\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3205\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m                 \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m             )\n\u001b[0;32m    190\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'res_merge.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def column_name(df):\n",
    "    # 첫 행 살리기\n",
    "    first = list (df.columns)\n",
    "    if first[0] == 0:\n",
    "        pass\n",
    "    else:\n",
    "        df.loc[0] = first\n",
    "        for val in first:\n",
    "            if 'Unnamed' in val:\n",
    "                x = first.index (val)\n",
    "                first[x] = np.nan\n",
    "        df.loc[0] = first\n",
    "    df = df.fillna ('')\n",
    "    sem_rgx = re.compile (r'[Q][A-Z]{3}')  # semantic tagset\n",
    "    syn_rgx = re.compile (r'[Y][A-Z]{3}')  # syntactic tagset\n",
    "    dom_rgx = re.compile (r'[X]{1}[ABCDEFGHIJKLMNOPQRSTUVWYZ]{3}')  # domain tagset\n",
    "    ent_rgx = re.compile (r'[X]{2}[A-Z]{2}')  # entity tagset\n",
    "    mor_rgx = re.compile (r'[A-Z]{3}')  # morph tagset\n",
    "\n",
    "    # 컬럼의 총 개수를 l에 저장한다.\n",
    "    # 컬럼의 개수 만큼 lemma와 category뒤에 lemma와 category개수인 2를 뺀만큼\n",
    "    # ''를 추가해 주어 해당 컬럼 개수 만큼의 리스트 col_nme을 만들어 준다.\n",
    "    l = len (df.columns)\n",
    "    col_nme = ['Lemma', 'Category']\n",
    "    for i in range (l - 2):\n",
    "        col_nme.append ('')\n",
    "    # sem =>SemInfo 뒤에 붙을 숫자\n",
    "    # syn =>SynInfo 뒤에 붙을 숫자\n",
    "    # dom =>DomInfo 뒤에 붙을 숫자\n",
    "    # ent =>EntInfo 뒤에 붙을 숫자\n",
    "    # mor =>MorInfo 뒤에 붙을 숫자\n",
    "    sem = 1\n",
    "    syn = 1\n",
    "    dom = 1\n",
    "    ent = 1\n",
    "    mor = 1\n",
    "\n",
    "    # x를 컬럼의 개수 만큼의 숫자로 지정해 준다.\n",
    "    # col_val은 해당 df의 열을 리스트화 시켜준 것이다.\n",
    "    for x in range (0, l):\n",
    "        col_val = df.iloc[:, x].tolist ()\n",
    "        # cnt가 0이면 일치하는 값을 못 찾았다는 의미로 해석(ex 모두 빈칸인 열을 만났을 때)\n",
    "        # 밑에서 cnt == 0 일때 앞에 정보를 보고 빈칸의 정보를 수정할 때 사용한다.\n",
    "        cnt = 0\n",
    "        # k로 col_val의 리스트 요소들을 하나씩 지정해주면서\n",
    "        # k가 sem_rgx, syn_rgx, dom_rgx, ent_rgx, mor_rgx에 해당되면\n",
    "        # 컬럼에 일치하는 값이 있었다는 의미로 cnt를 1 증가시켜 주고\n",
    "        # Info뒤에 붙을 숫자를 1씩 증가시켜 주고\n",
    "        # 비효율적인 탐색을 막기 위해 바로 break시켜준다.\n",
    "        for k in col_val:\n",
    "            if sem_rgx.match (k):\n",
    "                col_nme[x] = 'SemInfo' + str (sem)\n",
    "                sem += 1\n",
    "                cnt += 1\n",
    "                break\n",
    "\n",
    "            elif syn_rgx.match (k):\n",
    "                col_nme[x] = 'SynInfo' + str (syn)\n",
    "                syn += 1\n",
    "                cnt += 1\n",
    "                break\n",
    "\n",
    "            elif dom_rgx.match (k):\n",
    "                col_nme[x] = 'DomInfo' + str (dom)\n",
    "                dom += 1\n",
    "                cnt += 1\n",
    "                break\n",
    "\n",
    "            elif ent_rgx.match (k):\n",
    "                col_nme[x] = 'EntInfo' + str (ent)\n",
    "                ent += 1\n",
    "                cnt += 1\n",
    "                break\n",
    "\n",
    "            elif mor_rgx.match (k):\n",
    "                col_nme[x] = 'MorInfo' + str (mor)\n",
    "                mor += 1\n",
    "                cnt += 1\n",
    "                break\n",
    "\n",
    "        # 만약 위에서 일치하는 값을 못찾았을 때(ex 모두 빈칸인 열이었을 때)\n",
    "        # cnt는 0이므로 앞에 col_nme의 정보를 보고\n",
    "        # 해당 정보와 일치하는 정보의 Info숫자를 증가시켜준 값을 해당 리스트 위치에 저장해준다.\n",
    "        if cnt == 0:\n",
    "            if 'Sem' in col_nme[x - 1]:\n",
    "                col_nme[x] = 'SemInfo' + str (sem)\n",
    "                sem += 1\n",
    "\n",
    "            elif 'Syn' in col_nme[x - 1]:\n",
    "                col_nme[x] = 'SynInfo' + str (syn)\n",
    "                syn += 1\n",
    "\n",
    "            elif 'Dom' in col_nme[x - 1]:\n",
    "                col_nme[x] = 'DomInfo' + str (dom)\n",
    "                dom += 1\n",
    "\n",
    "            elif 'Ent' in col_nme[x - 1]:\n",
    "                col_nme[x] = 'EntInfo' + str (ent)\n",
    "                ent += 1\n",
    "\n",
    "            elif 'Mor' in col_nme[x - 1]:\n",
    "                col_nme[x] = 'MorInfo' + str (mor)\n",
    "                mor += 1\n",
    "\n",
    "    df.columns = col_nme\n",
    "\n",
    "    return df\n",
    "def Divide(korean_word):\n",
    "    CHOSUNG_LIST = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    # 중성 리스트. 00 ~ 20\n",
    "    JUNGSUNG_LIST = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ',\n",
    "                     'ㅣ']\n",
    "    # 종성 리스트. 00 ~ 27 + 1(1개 없음)\n",
    "    JONGSUNG_LIST = [' ', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ',\n",
    "                     'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "    r_lst = []\n",
    "    for w in list(korean_word.strip()):\n",
    "        ## 영어인 경우 구분해서 작성함.\n",
    "        if '가'<=w<='힣':\n",
    "            ## 588개 마다 초성이 바뀜.\n",
    "            ch1 = (ord(w) - ord('가'))//588\n",
    "            ## 중성은 총 28가지 종류\n",
    "            ch2 = ((ord(w) - ord('가')) - (588*ch1)) // 28\n",
    "            ch3 = (ord(w) - ord('가')) - (588*ch1) - 28*ch2\n",
    "            r_lst.append([CHOSUNG_LIST[ch1], JUNGSUNG_LIST[ch2], JONGSUNG_LIST[ch3]])\n",
    "        else:\n",
    "            r_lst.append([w])\n",
    "    return r_lst\n",
    "def check_info(word):\n",
    "    sem_rgx = re.compile (r'[Q][A-Z]{3}')  # semantic tagset\n",
    "    syn_rgx = re.compile (r'[Y][A-Z]{3}')  # syntactic tagset\n",
    "    dom_rgx = re.compile (r'[X]{1}[ABCDEFGHIJKLMNOPQRSTUVWYZ]{3}')  # domain tagset\n",
    "    ent_rgx = re.compile (r'[X]{2}[A-Z]{2}')  # entity tagset\n",
    "    mor_rgx = re.compile (r'[A-Z]{3}')  # morph tagset\n",
    "\n",
    "    if sem_rgx.match (word):\n",
    "        return 'SenInfo'\n",
    "    elif syn_rgx.match (word):\n",
    "        return 'SynInfo'\n",
    "    elif dom_rgx.match (word):\n",
    "        return 'DomInfo'\n",
    "    elif ent_rgx.match (word):\n",
    "        return 'EntInfo'\n",
    "    elif mor_rgx.match (word):\n",
    "        return 'MorInfo'\n",
    "def merge():\n",
    "    # 들어오는 데이터를 병합할 변수\n",
    "\n",
    "    merge_data = pd.read_csv(r\"E:\\Programming\\python\\NLP\\DecoLexO\\DecoLexO\\example\\Merge1.csv\", header=None, encoding='utf-8-sig')\n",
    "    merge_data = column_name(merge_data)\n",
    "    data_files = [r'E:\\Programming\\python\\NLP\\DecoLexO\\DecoLexO\\example\\Merge2.csv', r'E:\\Programming\\python\\NLP\\DecoLexO\\DecoLexO\\example\\Merge3.csv']\n",
    "    data_df_list = []\n",
    "    for i in data_files:\n",
    "        temp_data = pd.read_csv(i, header=None, encoding='utf-8-sig')\n",
    "        temp_data = column_name(temp_data)\n",
    "        data_df_list.append(temp_data)\n",
    "    # 두 번째부터는 들어오는 데이터를 data변수에 저장하고\n",
    "    # 이전에 저장해둔 merge_data와 data를 concat으로 병합한 뒤\n",
    "    # 두 데이터를 sort_values로 정렬을 시켜준다.\n",
    "    for data in data_df_list:\n",
    "        merge_data = pd.concat ([merge_data, data], ignore_index=True)\n",
    "    merge_data = merge_data.sort_values (by='Lemma')\n",
    "    merge_data = merge_data.reset_index ()\n",
    "    merge_data = merge_data.fillna('')\n",
    "    del merge_data['index']\n",
    "    cnt = 1\n",
    "    # 사용자가 입력할 col_name.\n",
    "    # gui상에서 .text()로 입력받는다.\n",
    "    col_data = 'MorInfo1'\n",
    "\n",
    "    # 우선시 되는 데이터 : my_data, 지워져도 되는 데이터 del_data\n",
    "    # gui상에서 .text()로 입력받는다.\n",
    "    rules = [['ZNZ','ZNW'],['ZNE','ZNW']]\n",
    "    del_row_list = []\n",
    "    for rule in rules:\n",
    "        my_data = rule[0]\n",
    "        del_data = rule[1]\n",
    "        for i in range (0, len(merge_data) - 1):\n",
    "            for j in range (i + 1, i + cnt + 1):\n",
    "                # first에는 i번째 단어를 음절별로 나누어서 저장하고\n",
    "                # second에는 j번째(i다음 단어)를 음절로 나누어서 저장한다.\n",
    "                first = Divide (merge_data.loc[i, 'Lemma'])\n",
    "                second = Divide (merge_data.loc[j, 'Lemma'])\n",
    "                if first[0] != second[0]:\n",
    "                    break\n",
    "                    # 만약 단어의 앞글자가 같다면 아래 코드를 실행한다.\n",
    "                else:\n",
    "                    # 앞글자는 같지만 단어가 다르면 for문을 종료한다.\n",
    "                    if first != second:\n",
    "                        break\n",
    "\n",
    "                    # 앞글자가 같고 단어까지 같으면 입력 받은 mydata가 i번째에 있는지 아님 j(i+1)번째 있는지 확인한다.\n",
    "                    else:\n",
    "                        if my_data == merge_data.loc[i, col_data] and del_data == merge_data.loc[j, col_data]:\n",
    "                            # reduplication.append(merge_data.iloc[i].values.tolist())\n",
    "                            # reduplication.append(merge_data.iloc[j].values.tolist())\n",
    "\n",
    "                            # stem_data에는 i번째 단어 정보를 리스트형태로 저장하고\n",
    "                            # follow_data에는 j번째 (i+1)번째 단어 정보를 리스트 형태로 저장한다.\n",
    "                            # 변수 x는 follow_data를 돌면서 follow_data요소가 stem_data에 정보가 없으면\n",
    "                            # 그 정보들 check_info()함수에 넘겨서 정보를 col에 저장해준다(ex. MorInfo)\n",
    "                            # y는 merge_data의 colum 정보들을 돌면서\n",
    "                            # check_info로 입력받은 정보가 들어있는 column이 처음으로 빈칸이 나오는 장소에\n",
    "                            # stem_data에 들어있지 않은 정보(follow_data)를 merge_data에 넣어준다.\n",
    "                            stem_data1 = merge_data.iloc[i].values.tolist ()\n",
    "                            follow_data1 = merge_data.iloc[j].values.tolist ()\n",
    "                            for x in range (4, len (follow_data1) - 1):\n",
    "                                if follow_data1[x] not in stem_data1:\n",
    "                                    col = check_info (follow_data1[x])\n",
    "                                    col_nme = merge_data.columns\n",
    "                                    for y in range (4, len(col_nme)):\n",
    "                                        if col in col_nme[y] and merge_data.loc[j, col_nme[y]] == '':\n",
    "                                            merge_data.loc[j, col_nme[y]] = follow_data1[x]\n",
    "                                            break\n",
    "                            del_row_list.append(j)\n",
    "\n",
    "                        elif my_data == merge_data.loc[i, col_data] and del_data == merge_data.loc[j, col_data]:\n",
    "                            # reduplication.append(merge_data.iloc[i].values.tolist())\n",
    "                            # reduplication.append(merge_data.iloc[j].values.tolist())\n",
    "                            stem_data2 = merge_data.iloc[j].values.tolist ()\n",
    "                            follow_data2 = merge_data.iloc[i].values.tolist ()\n",
    "                            for x in range (4, len (follow_data2) - 1):\n",
    "                                if follow_data2[x] not in stem_data2:\n",
    "                                    col = check_info (follow_data2[x])\n",
    "                                    col_nme = merge_data.columns\n",
    "                                    for y in range (4, len (col_nme)):\n",
    "                                        if col in col_nme[y] and merge_data.loc[j, col_nme[y]] == '':\n",
    "                                            merge_data.loc[j, col_nme[y]] = follow_data2[x]\n",
    "                                            break\n",
    "                            del_row_list.append(j)\n",
    "                        else:\n",
    "                            pass\n",
    "    del_temp = []\n",
    "    for i in del_row_list:\n",
    "        del_temp.append(merge_data.iloc[i].tolist())\n",
    "    del_df = pd.DataFrame(del_temp)\n",
    "    del_df = column_name(del_df)\n",
    "    del_df.to_csv(\"res_del_merge.csv\", header=False, index=False, na_rep='', encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "    merge_data = merge_data.drop(del_row_list, 0)\n",
    "    merge_data = merge_data.reset_index()\n",
    "    del merge_data[\"index\"]\n",
    "\n",
    "    return merge_data.to_csv(\"res_merge.csv\", header=False, index=False, na_rep='', encoding='utf-8-sig')\n",
    "merge()\n",
    "print('RM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}